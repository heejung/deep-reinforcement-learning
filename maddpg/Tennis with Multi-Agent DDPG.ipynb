{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition with MADDPG\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Start the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"./Tennis.app\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.         -6.65278625 -1.5\n",
      " -0.          0.          6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Performance of a Random Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: 0.04500000085681677\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n",
      "Total score (averaged over agents) this episode: -0.004999999888241291\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Hyperparameters\n",
    "\n",
    "buffer_size = int(1e5)\n",
    "\n",
    "seed = 3\n",
    "\n",
    "batch_size = 128 \n",
    "update_every = 1 \n",
    "gamma = 1.0 # discount \n",
    "tau = 0.001\n",
    "actor_learning_rate = 1e-4\n",
    "critic_learning_rate = 1e-4 \n",
    "critic_weight_decay = 0\n",
    "fc1_dim = 256 \n",
    "fc2_dim = 128 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Class Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actor - Policy\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, action_size, state_size, seed, fc1_dim=400, fc2_dim=300):\n",
    "        super(Actor, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_dim)\n",
    "        self.fc2 = nn.Linear(fc1_dim, fc2_dim)\n",
    "        self.output = nn.Linear(fc2_dim, action_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.reset_parameters(fc1_dim, fc2_dim)\n",
    "        \n",
    "    def reset_parameters(self, fc1_dim, fc2_dim):\n",
    "        self.fc1.weight.data.uniform_(-1./math.sqrt(fc1_dim), 1./math.sqrt(fc1_dim))\n",
    "        self.fc2.weight.data.uniform_(-1./math.sqrt(fc2_dim), 1./math.sqrt(fc2_dim))\n",
    "        self.output.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        self.output.bias.data.uniform_(3e-4, 3e-4)\n",
    "        \n",
    "    def forward(self, states):\n",
    "        x = states\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.tanh(self.output(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Critic - Q Value\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, action_size, state_size, seed, fc1_dim=400, fc2_dim=300):\n",
    "        super(Critic, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size, fc1_dim)\n",
    "        self.fc2 = nn.Linear(fc1_dim+action_size, fc2_dim)\n",
    "        self.output = nn.Linear(fc2_dim, 1)\n",
    "        self.reset_parameters(fc1_dim, fc2_dim+action_size)\n",
    "        \n",
    "    def reset_parameters(self, fc1_dim, fc2_dim):\n",
    "        self.fc1.weight.data.uniform_(-1./math.sqrt(fc1_dim), 1./math.sqrt(fc1_dim))\n",
    "        self.fc2.weight.data.uniform_(-1./math.sqrt(fc2_dim), 1./math.sqrt(fc2_dim))\n",
    "        self.output.weight.data.uniform_(-3e-3, 3e-3)\n",
    "        self.output.bias.data.uniform_(3e-4, 3e-4)\n",
    "        \n",
    "    def forward(self, states, actions):\n",
    "        xs = states\n",
    "        xs = F.relu(self.fc1(xs))\n",
    "        x = torch.cat((xs, actions), dim=1)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ReplayBuffer\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed, device):\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        self.device = device\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(self.device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(self.device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(self.device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(self.device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(self.device)\n",
    "  \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "    \n",
    "memory = ReplayBuffer(action_size, buffer_size, batch_size, seed, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noise\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "class OrnsteinUhlenbeckNoise:\n",
    "    \"\"\"Ornstein-Uhlenbeck process.\"\"\"\n",
    "\n",
    "    def __init__(self, size, seed, mu=0., theta=0.15, sigma=0.2):\n",
    "        \"\"\"Initialize parameters and noise process.\"\"\"\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.size = size\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.seed = random.seed(seed)\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        x = self.state        \n",
    "        dx = self.theta * (self.mu - x) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = x + dx\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPGAgent\n",
    "class DDPGAgent:\n",
    "    def __init__(self, action_size, state_size, seed, fc1_dim, fc2_dim, actor_learning_rate, critic_learning_rate,\n",
    "                 critic_weight_decay, device, gamma, tau):\n",
    "        \"\"\"Initialize all parameters.\"\"\"\n",
    "\n",
    "        self.actor_local = Actor(action_size, state_size, seed, fc1_dim, fc2_dim).to(device)\n",
    "        self.actor_target = Actor(action_size, state_size, seed, fc1_dim, fc2_dim).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor_local.parameters(), lr=actor_learning_rate)\n",
    "        \n",
    "        self.critic_local = Critic(action_size, state_size, seed, fc1_dim, fc2_dim).to(device)\n",
    "        self.critic_target = Critic(action_size, state_size, seed, fc1_dim, fc2_dim).to(device)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic_local.parameters(), lr=critic_learning_rate,\n",
    "                                                 weight_decay=critic_weight_decay)\n",
    "        \n",
    "        self.noise = OrnsteinUhlenbeckNoise(action_size, seed)\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        self.noise.reset()\n",
    "        \n",
    "    def act(self, states, device, train_mode=False):\n",
    "        states = torch.from_numpy(states).float().to(device)\n",
    "        self.actor_local.eval()\n",
    "        with torch.no_grad():\n",
    "            actions = self.actor_local(states).cpu().data.numpy()\n",
    "        self.actor_local.train()\n",
    "        \n",
    "        if train_mode:\n",
    "            actions += self.noise.sample()\n",
    "        return np.clip(actions, -1, 1)\n",
    "    \n",
    "    def learn(self, experiences):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "\n",
    "        # Update Critic\n",
    "        next_actions = self.actor_target(next_states)\n",
    "        q_targets_next = self.critic_target(next_states, next_actions)\n",
    "        q_targets = rewards + (self.gamma * q_targets_next * (1 - dones))\n",
    "        q_expected = self.critic_local(states, actions)\n",
    "        critic_loss = F.mse_loss(q_expected, q_targets)\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "        \n",
    "        # Update Actor\n",
    "        actions_expected = self.actor_local(states)\n",
    "        actor_loss = -self.critic_local(states, actions_expected).mean()\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.actor_local, self.actor_target, self.tau)\n",
    "        self.soft_update(self.critic_local, self.critic_target, self.tau)\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1.0-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MADDPG\n",
    "class MADDPG:\n",
    "    def __init__(self, action_size, state_size, batch_size, seed, fc1_dim, fc2_dim, actor_learning_rate,\n",
    "                 critic_learning_rate, critic_weight_decay, memory, device, num_agents, update_every, gamma, tau):\n",
    "        super(MADDPG, self).__init__()\n",
    "\n",
    "        self.ddpg_agents = [DDPGAgent(action_size, state_size, seed, fc1_dim, fc2_dim, actor_learning_rate, \n",
    "                                      critic_learning_rate, critic_weight_decay, device, gamma,\n",
    "                                      tau) for _ in range(num_agents)]\n",
    "\n",
    "        self.memory = memory          \n",
    "        self.update_every = update_every\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.t_step = 0\n",
    "     \n",
    "    def reset(self):\n",
    "        for agent in self.ddpg_agents:\n",
    "            agent.reset()\n",
    "\n",
    "    def act(self, all_states, device, train_mode=False):\n",
    "        \"\"\"get actions from all agents in the MADDPG object\"\"\"\n",
    "        actions = [agent.act(np.expand_dims(states, axis=0), device,\n",
    "                             train_mode) for agent, states in zip(self.ddpg_agents, all_states)]\n",
    "        return actions\n",
    "    \n",
    "    def step(self, states, actions, rewards, next_states, dones):\n",
    "        # Save experience in replay memory\n",
    "        for state, action, reward, next_state, done in zip(states, actions, rewards, next_states, dones):\n",
    "            self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                for agent in self.ddpg_agents:\n",
    "                    experiences = self.memory.sample()\n",
    "                    \n",
    "                    agent.learn(experiences)\n",
    "\n",
    "maddpg = MADDPG(action_size, state_size, batch_size, seed, fc1_dim, fc2_dim, actor_learning_rate,\n",
    "                critic_learning_rate, critic_weight_decay, memory, device, num_agents, update_every, gamma, tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start:  2019-07-16 05:41:30.585498\n",
      "No,Score,  Avg,  Min, Time, Buffer\n",
      " 010, 0.00, 0.01, -0.01, 0.48 338\n",
      " 020, 0.00, 0.01, -0.01, 0.29 622\n",
      " 030, 0.00, 0.00, -0.01, 0.28 906\n",
      " 040, 0.00, 0.00, -0.01, 0.28 1190\n",
      " 050, 0.00, 0.00, -0.01, 0.30 1474\n",
      " 060, 0.00, 0.00, -0.01, 0.30 1758\n",
      " 070, 0.00, 0.00, -0.01, 0.30 2042\n",
      " 080, 0.00, 0.00, -0.01, 0.31 2326\n",
      " 090, 0.00, 0.00, -0.01, 0.29 2610\n",
      " 100, 0.00, 0.00, -0.01, 0.29 2894\n",
      " 110, 0.00, 0.00, -0.01, 0.30 3178\n",
      " 120, 0.00, 0.00, -0.01, 0.31 3462\n",
      " 130, 0.00, 0.00, -0.01, 0.31 3746\n",
      " 140, 0.00, 0.00, -0.01, 0.30 4030\n",
      " 150, 0.00, 0.00, -0.01, 0.30 4314\n",
      " 160, 0.00, 0.00, -0.01, 0.30 4598\n",
      " 170, 0.00, 0.00, -0.01, 0.30 4882\n",
      " 180, 0.00, 0.00, -0.01, 0.29 5166\n",
      " 190, 0.00, 0.00, -0.01, 0.31 5450\n",
      " 200, 0.00, 0.00, -0.01, 0.29 5734\n",
      " 210, 0.00, 0.00, -0.01, 0.30 6018\n",
      " 220, 0.00, 0.00, -0.01, 0.31 6302\n",
      " 230, 0.00, 0.00, -0.01, 0.29 6586\n",
      " 240, 0.00, 0.00, -0.01, 0.31 6870\n",
      " 250, 0.00, 0.00, -0.01, 0.31 7154\n",
      " 260, 0.00, 0.00, -0.01, 0.32 7438\n",
      " 270, 0.00, 0.00, -0.01, 0.29 7722\n",
      " 280, 0.00, 0.00, -0.01, 0.30 8006\n",
      " 290, 0.00, 0.00, -0.01, 0.31 8290\n",
      " 300, 0.00, 0.00, -0.01, 0.29 8574\n",
      " 310, 0.00, 0.00, -0.01, 0.30 8858\n",
      " 320, 0.00, 0.00, -0.01, 0.31 9142\n",
      " 330, 0.00, 0.00, -0.01, 0.29 9426\n",
      " 340, 0.00, 0.00, -0.01, 0.29 9718\n",
      " 350, 0.00, 0.00, -0.01, 0.36 10008\n",
      " 360, 0.00, 0.00, -0.01, 0.29 10292\n",
      " 370, 0.00, 0.00, -0.01, 0.30 10576\n",
      " 380, 0.00, 0.00, -0.01, 0.33 10878\n",
      " 390, 0.00, 0.00, -0.01, 0.28 11164\n",
      " 400, 0.00, 0.00, -0.01, 0.30 11448\n",
      " 410, 0.00, 0.00, -0.01, 0.29 11732\n",
      " 420, 0.00, 0.00, -0.01, 0.31 12016\n",
      " 430, 0.00, 0.00, -0.01, 0.30 12300\n",
      " 440, 0.00, 0.00, -0.01, 0.30 12584\n",
      " 450, 0.00, 0.00, -0.01, 0.30 12868\n",
      " 460, 0.00, 0.00, -0.01, 0.30 13152\n",
      " 470, 0.00, 0.00, -0.01, 0.30 13436\n",
      " 480, 0.00, 0.00, -0.01, 0.31 13720\n",
      " 490, 0.00, 0.00, -0.01, 0.29 14004\n",
      " 500, 0.00, 0.00, -0.01, 0.28 14348\n",
      " 510, 0.00, 0.00, -0.01, 0.29 14632\n",
      " 520, 0.00, 0.00, -0.01, 0.31 14916\n",
      " 530, 0.00, 0.00, -0.01, 0.31 15200\n",
      " 540, 0.00, 0.00, -0.01, 0.29 15484\n",
      " 550, 0.00, 0.00, -0.01, 0.29 15768\n",
      " 560, 0.00, 0.00, -0.01, 0.30 16052\n",
      " 570, 0.00, 0.00, -0.01, 0.31 16336\n",
      " 580, 0.00, 0.00, -0.01, 0.32 16620\n",
      " 590, 0.00, 0.00, -0.01, 0.30 16904\n",
      " 600, 0.00, 0.00, -0.01, 0.30 17188\n",
      " 610, 0.00, 0.00, -0.01, 0.31 17544\n",
      " 620, 0.00, 0.00, -0.01, 0.31 17848\n",
      " 630, 0.00, 0.00, -0.01, 0.33 18144\n",
      " 640, 0.00, 0.00, -0.01, 0.31 18428\n",
      " 650, 0.00, 0.00, -0.01, 0.33 18712\n",
      " 660, 0.00, 0.00, -0.01, 0.32 19060\n",
      " 670, 0.00, 0.00, -0.01, 0.32 19344\n",
      " 680, 0.00, 0.00, -0.01, 0.31 19636\n",
      " 690, 0.00, 0.00, -0.01, 0.30 19956\n",
      " 700, 0.00, 0.00, -0.01, 0.31 20240\n",
      " 710, 0.00, 0.00, -0.01, 0.30 20524\n",
      " 720, 0.00, 0.00, -0.01, 0.30 20808\n",
      " 730, 0.00, 0.00, -0.01, 0.30 21092\n",
      " 740, 0.00, 0.00, -0.01, 0.33 21452\n",
      " 750, 0.00, 0.00, -0.01, 0.30 21736\n",
      " 760, 0.00, 0.00, -0.01, 0.30 22090\n",
      " 770, 0.00, 0.00, -0.01, 0.30 22374\n",
      " 780, 0.00, 0.00, -0.01, 0.29 22658\n",
      " 790, 0.00, 0.00, -0.01, 0.31 22942\n",
      " 800, 0.00, 0.00, -0.01, 0.30 23226\n",
      " 810, 0.00, 0.00, -0.01, 0.30 23510\n",
      " 820, 0.00, 0.00, -0.01, 0.30 23794\n",
      " 830, 0.00, 0.00, -0.01, 0.29 24078\n",
      " 840, 0.00, 0.00, -0.01, 0.30 24362\n",
      " 850, 0.00, 0.00, -0.01, 0.29 24646\n",
      " 860, 0.00, 0.00, -0.01, 0.31 24934\n",
      " 870, 0.00, 0.00, -0.01, 0.31 25282\n",
      " 880, 0.00, 0.00, -0.01, 0.30 25674\n",
      " 890, 0.20, 0.01, 0.19, 1.95 26232\n",
      " 900, 0.00, 0.01, -0.01, 0.29 26784\n",
      " 910, 0.00, 0.01, -0.01, 0.31 27332\n",
      " 920, 0.00, 0.01, -0.01, 0.31 27622\n",
      " 930, 0.00, 0.02, -0.01, 0.39 28190\n",
      " 940, 0.00, 0.02, -0.01, 0.30 28554\n",
      " 950, 0.00, 0.02, -0.01, 0.31 28920\n",
      " 960, 0.00, 0.02, -0.01, 0.30 29278\n",
      " 970, 0.10, 0.02, 0.09, 1.13 29958\n",
      " 980, 0.10, 0.02, 0.09, 1.17 30330\n",
      " 990, 0.00, 0.02, -0.01, 0.36 30660\n",
      " 1000, 0.00, 0.02, -0.01, 0.30 31028\n",
      " 1010, 0.00, 0.02, -0.01, 0.30 31398\n",
      " 1020, 0.00, 0.02, -0.01, 0.35 31692\n",
      " 1030, 0.00, 0.02, -0.01, 0.33 32194\n",
      " 1040, 0.00, 0.02, -0.01, 0.32 32626\n",
      " 1050, 0.00, 0.02, -0.01, 0.31 32924\n",
      " 1060, 0.00, 0.02, -0.01, 0.29 33676\n",
      " 1070, 0.00, 0.02, -0.01, 0.29 34124\n",
      " 1080, 0.00, 0.02, -0.01, 0.35 34420\n",
      " 1090, 0.00, 0.02, -0.01, 0.29 34816\n",
      " 1100, 0.00, 0.02, -0.01, 0.31 35174\n",
      " 1110, 0.10, 0.02, -0.01, 0.75 35650\n",
      " 1120, 0.00, 0.02, -0.01, 0.32 36076\n",
      " 1130, 0.00, 0.02, -0.01, 0.30 36366\n",
      " 1140, 0.00, 0.02, -0.01, 0.29 36810\n",
      " 1150, 0.00, 0.02, -0.01, 0.38 37182\n",
      " 1160, 0.00, 0.02, -0.01, 0.31 37740\n",
      " 1170, 0.00, 0.02, -0.01, 0.29 38070\n",
      " 1180, 0.00, 0.02, -0.01, 0.33 38392\n",
      " 1190, 0.00, 0.02, -0.01, 0.31 39286\n",
      " 1200, 0.00, 0.03, -0.01, 0.33 39798\n",
      " 1210, 0.00, 0.02, -0.01, 0.33 40134\n",
      " 1220, 0.00, 0.03, -0.01, 0.52 40744\n",
      " 1230, 0.00, 0.03, -0.01, 0.30 41140\n",
      " 1240, 0.10, 0.03, -0.01, 0.67 41586\n",
      " 1250, 0.00, 0.04, -0.01, 0.30 42060\n",
      " 1260, 0.00, 0.03, -0.01, 0.35 42460\n",
      " 1270, 0.00, 0.05, -0.01, 0.29 43418\n",
      " 1280, 0.00, 0.05, -0.01, 0.30 43766\n",
      " 1290, 0.00, 0.04, -0.01, 0.30 44096\n",
      " 1300, 0.00, 0.04, -0.01, 0.32 44524\n",
      " 1310, 0.00, 0.04, -0.01, 0.50 44858\n",
      " 1320, 0.00, 0.04, -0.01, 0.33 45210\n",
      " 1330, 0.00, 0.04, -0.01, 0.31 45584\n",
      " 1340, 0.10, 0.03, -0.01, 0.66 45932\n",
      " 1350, 0.00, 0.03, -0.01, 0.31 46254\n",
      " 1360, 0.10, 0.03, 0.09, 1.01 46642\n",
      " 1370, 0.10, 0.02, 0.09, 0.89 47014\n",
      " 1380, 0.10, 0.02, -0.01, 0.71 47468\n",
      " 1390, 0.10, 0.02, -0.01, 0.67 47882\n",
      " 1400, 0.00, 0.02, -0.01, 0.31 48262\n",
      " 1410, 0.00, 0.02, -0.01, 0.31 48644\n",
      " 1420, 0.10, 0.02, -0.01, 0.69 49104\n",
      " 1430, 0.19, 0.03, 0.10, 1.48 49692\n",
      " 1440, 0.10, 0.03, -0.01, 0.63 50174\n",
      " 1450, 0.00, 0.04, -0.01, 0.36 50622\n",
      " 1460, 0.00, 0.03, -0.01, 0.31 50974\n",
      " 1470, 0.00, 0.04, -0.01, 0.28 51466\n",
      " 1480, 0.10, 0.04, -0.01, 0.74 52176\n",
      " 1490, 0.10, 0.05, -0.01, 0.65 52994\n",
      " 1500, 0.10, 0.05, -0.01, 0.94 53572\n",
      " 1510, 0.00, 0.06, -0.01, 0.31 54332\n",
      " 1520, 0.20, 0.06, 0.09, 1.53 54986\n",
      " 1530, 0.00, 0.06, -0.01, 0.33 55574\n",
      " 1540, 0.09, 0.07, 0.00, 0.36 56496\n",
      " 1550, 0.10, 0.07, -0.01, 0.69 56992\n",
      " 1560, 0.10, 0.08, -0.01, 0.67 57576\n",
      " 1570, 0.00, 0.08, -0.01, 0.30 58002\n",
      " 1580, 0.00, 0.07, -0.01, 0.31 58476\n",
      " 1590, 0.09, 0.07, 0.00, 0.81 59138\n",
      " 1600, 0.10, 0.07, -0.01, 0.45 59734\n",
      " 1610, 0.00, 0.06, -0.01, 0.37 60488\n",
      " 1620, 0.30, 0.06, 0.29, 3.19 61134\n",
      " 1630, 0.10, 0.06, -0.01, 0.66 61798\n",
      " 1640, 0.09, 0.06, 0.00, 0.65 62412\n",
      " 1650, 0.09, 0.06, 0.00, 0.65 63242\n",
      " 1660, 0.19, 0.07, 0.10, 1.35 64026\n",
      " 1670, 0.10, 0.08, -0.01, 0.69 64758\n",
      " 1680, 0.10, 0.08, 0.09, 1.21 65586\n",
      " 1690, 0.30, 0.09, 0.19, 2.47 66502\n",
      " 1700, 0.10, 0.10, -0.01, 0.98 67558\n",
      " 1710, 0.00, 0.10, -0.01, 0.33 68462\n",
      " 1720, 0.10, 0.11, -0.01, 0.70 69532\n",
      " 1730, 0.40, 0.12, 0.29, 3.13 71000\n",
      " 1740, 0.90, 0.14, 0.89, 7.74 72882\n",
      " 1750, 0.10, 0.14, -0.01, 0.65 73916\n",
      " 1760, 0.10, 0.14, -0.01, 0.68 74878\n",
      " 1770, 0.10, 0.15, -0.01, 0.72 75872\n",
      " 1780, 0.30, 0.16, 0.19, 2.63 77452\n",
      " 1790, 0.20, 0.16, 0.09, 1.45 78912\n",
      " 1800, 0.10, 0.17, 0.09, 1.15 80482\n",
      " 1810, 0.20, 0.17, 0.09, 1.55 81764\n",
      " 1820, 0.09, 0.17, 0.00, 0.64 82978\n",
      " 1830, 0.40, 0.18, 0.39, 3.86 85018\n",
      " 1840, 0.10, 0.16, -0.01, 0.77 85884\n",
      " 1850, 0.10, 0.17, -0.01, 0.76 87378\n",
      " 1860, 0.50, 0.18, 0.49, 4.48 89918\n",
      " 1870, 0.00, 0.20, -0.01, 0.33 92046\n",
      " 1880, 0.10, 0.19, -0.01, 0.71 92982\n",
      " 1890, 0.80, 0.21, 0.69, 6.52 96026\n",
      " 1900, 0.10, 0.22, -0.01, 0.67 98214\n",
      " 1910, 0.50, 0.23, 0.49, 4.49 100000\n",
      " 1920, 1.50, 0.30, 1.39, 12.44 100000\n",
      " 1930, 0.10, 0.33, 0.09, 1.12 100000\n",
      " 1940, 0.90, 0.36, 0.89, 8.00 100000\n",
      " 1950, 0.10, 0.36, 0.09, 1.13 100000\n",
      " 1960, 0.00, 0.38, -0.01, 0.31 100000\n",
      " 1970, 1.30, 0.43, 1.29, 11.18 100000\n",
      " 1980, 0.30, 0.45, 0.29, 2.80 100000\n",
      " 1990, 1.60, 0.49, 1.49, 13.85 100000\n",
      "\n",
      "Solved in 1993 episodes!\tAvg Score: 0.51, time: 1492.98171210289\n",
      "End:  2019-07-16 06:06:23.570586\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from workspace_utils import keep_awake\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "\n",
    "avg_over = 100\n",
    "print_every = 10\n",
    "\n",
    "n_episodes=30000\n",
    "max_t=2000\n",
    "\n",
    "print('Start: ', datetime.datetime.now())\n",
    "scores_deque = deque(maxlen=avg_over)\n",
    "scores_global = []\n",
    "average_global = []\n",
    "min_global = []    \n",
    "best_avg = -np.inf\n",
    "\n",
    "scores_window = deque(maxlen=100)\n",
    "\n",
    "tic = time.time()\n",
    "print('\\rNo,Score,  Avg,  Min, Time, Buffer')\n",
    "for i_episode in keep_awake(range(1, n_episodes+1)):\n",
    "\n",
    "    maddpg.reset()\n",
    "    env_info = env.reset(train_mode=True)[brain_name]\n",
    "    states = env_info.vector_observations    \n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "\n",
    "    score_average = 0\n",
    "    timestep = time.time()\n",
    "    for i in range(max_t):\n",
    "        actions = maddpg.act(states, device, train_mode=True)\n",
    "        env_info = env.step(actions)[brain_name]\n",
    "        rewards = env_info.rewards\n",
    "        next_states = env_info.vector_observations\n",
    "        dones = env_info.local_done\n",
    "\n",
    "        maddpg.step(states, actions, rewards, next_states, dones)\n",
    "\n",
    "        scores += rewards\n",
    "        states = next_states\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    \n",
    "    score = np.max(scores)\n",
    "    \n",
    "    scores_deque.append(score)\n",
    "    score_average = np.mean(scores_deque)\n",
    "    scores_global.append(score)\n",
    "    average_global.append(score_average)  \n",
    "    min_global.append(np.min(scores))  \n",
    "\n",
    "    if i_episode % print_every == 0:\n",
    "        torch.save(maddpg.ddpg_agents[0].actor_local.state_dict(), str(i_episode) + '_checkpoint_actor.pth')\n",
    "        torch.save(maddpg.ddpg_agents[0].critic_local.state_dict(), str(i_episode) + '_checkpoint_critic.pth')\n",
    "        print('\\r {}, {:.2f}, {:.2f}, {:.2f}, {:.2f}'\\\n",
    "          .format(str(i_episode).zfill(3), score, score_average, \n",
    "                  np.min(scores), time.time() - timestep), len(memory), end=\"\\n\")\n",
    "    if  score_average > 0.5:            \n",
    "        toc = time.time()\n",
    "        print('\\nSolved in {:d} episodes!\\tAvg Score: {:.2f}, time: {}'.format(i_episode, score_average, toc-tic))\n",
    "        torch.save(maddpg.ddpg_agents[0].actor_local.state_dict(), str(i_episode) + '_checkpoint_actor.pth')\n",
    "        torch.save(maddpg.ddpg_agents[0].critic_local.state_dict(), str(i_episode) + '_checkpoint_critic.pth')\n",
    "        break\n",
    "\n",
    "print('End: ', datetime.datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(maddpg.ddpg_agents[0].actor_local.state_dict(), 'final_checkpoint_actor0.pth')\n",
    "torch.save(maddpg.ddpg_agents[0].critic_local.state_dict(), 'final_checkpoint_critic0.pth')\n",
    "torch.save(maddpg.ddpg_agents[1].actor_local.state_dict(), 'final_checkpoint_actor1.pth')\n",
    "torch.save(maddpg.ddpg_agents[1].critic_local.state_dict(), 'final_checkpoint_critic1.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f98cadd1128>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "scores = scores_global\n",
    "averages = average_global\n",
    "\n",
    "plt.plot(np.arange(1, len(scores)+1), scores)\n",
    "plt.plot(np.arange(1, len(averages)+1), averages)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.legend(['Score','Avg'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Load & Test our Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_copy(target, source):\n",
    "    for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "        target_param.data.copy_(param.data)\n",
    "\n",
    "def load(agent, actor_file, critic_file):\n",
    "    agent.actor_local.load_state_dict(torch.load(actor_file, map_location=lambda storage, loc: storage))\n",
    "    agent.critic_local.load_state_dict(torch.load(critic_file, map_location=lambda storage, loc: storage))\n",
    "    hard_copy(agent.actor_target, agent.actor_local)\n",
    "    hard_copy(agent.critic_target, agent.critic_local)\n",
    "\n",
    "agent = DDPGAgent(action_size, state_size, seed, fc1_dim, fc2_dim, actor_learning_rate, critic_learning_rate,\n",
    "                  critic_weight_decay, device, gamma, tau)\n",
    "load(agent, 'final_checkpoint_actor0.pth', 'final_checkpoint_critic0.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 1.1950000179931521\n",
      "Total score (averaged over agents) this episode: 1.6950000254437327\n",
      "Total score (averaged over agents) this episode: 2.600000038743019\n",
      "Total score (averaged over agents) this episode: 2.650000039488077\n",
      "Total score (averaged over agents) this episode: 1.145000017248094\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = agent.act(states, device, train_mode=False)\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When finished, you can close the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:drlnd]",
   "language": "python",
   "name": "conda-env-drlnd-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
